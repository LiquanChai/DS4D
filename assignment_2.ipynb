{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n",
      "2016-06-21/2016-06-21:18:34:02\n",
      "Finish: 2016-06-21/2016-06-21:18:34:02\n",
      "2016-06-21/2016-06-21:16:34:01\n",
      "Finish: 2016-06-21/2016-06-21:16:34:01\n",
      "2016-06-21/2016-06-21:21:34:01\n",
      "Finish: 2016-06-21/2016-06-21:21:34:01\n",
      "2016-06-21/2016-06-21:22:34:02\n",
      "Finish: 2016-06-21/2016-06-21:22:34:02\n",
      "2016-06-21/2016-06-21:07:34:01\n",
      "Finish: 2016-06-21/2016-06-21:07:34:01\n",
      "2016-06-21/2016-06-21:09:34:02\n",
      "Finish: 2016-06-21/2016-06-21:09:34:02\n",
      "2016-06-21/2016-06-21:13:34:01\n",
      "Finish: 2016-06-21/2016-06-21:13:34:01\n",
      "2016-06-21/2016-06-21:23:34:01\n",
      "Finish: 2016-06-21/2016-06-21:23:34:01\n",
      "2016-06-21/2016-06-21:14:34:01\n",
      "Finish: 2016-06-21/2016-06-21:14:34:01\n",
      "2016-06-21/2016-06-21:08:34:01\n",
      "Finish: 2016-06-21/2016-06-21:08:34:01\n",
      "2016-06-21/2016-06-21:19:34:01\n",
      "Finish: 2016-06-21/2016-06-21:19:34:01\n",
      "2016-06-21/2016-06-21:20:34:02\n",
      "Finish: 2016-06-21/2016-06-21:20:34:02\n",
      "2016-06-21/2016-06-21:05:34:01\n",
      "Finish: 2016-06-21/2016-06-21:05:34:01\n",
      "2016-06-21/2016-06-21:00:34:01\n",
      "Finish: 2016-06-21/2016-06-21:00:34:01\n",
      "2016-06-21/2016-06-21:11:34:01\n",
      "Finish: 2016-06-21/2016-06-21:11:34:01\n",
      "2016-06-21/2016-06-21:03:34:01\n",
      "Finish: 2016-06-21/2016-06-21:03:34:01\n",
      "2016-06-21/2016-06-21:12:34:01\n",
      "Finish: 2016-06-21/2016-06-21:12:34:01\n",
      "2016-06-21/2016-06-21:17:34:01\n",
      "Finish: 2016-06-21/2016-06-21:17:34:01\n",
      "2016-06-21/2016-06-21:06:34:01\n",
      "Finish: 2016-06-21/2016-06-21:06:34:01\n",
      "2016-06-21/2016-06-21:01:34:01\n",
      "Finish: 2016-06-21/2016-06-21:01:34:01\n",
      "2016-06-21/2016-06-21:10:34:01\n",
      "Finish: 2016-06-21/2016-06-21:10:34:01\n",
      "2016-06-21/2016-06-21:02:34:02\n",
      "Finish: 2016-06-21/2016-06-21:02:34:02\n",
      "2016-06-21/2016-06-21:15:34:01\n",
      "Finish: 2016-06-21/2016-06-21:15:34:01\n",
      "2016-06-21/2016-06-21:04:34:01\n",
      "Finish: 2016-06-21/2016-06-21:04:34:01\n",
      "2016-06-23/2016-06-23:10:34:01\n",
      "Finish: 2016-06-23/2016-06-23:10:34:01\n",
      "2016-06-23/2016-06-23:01:34:01\n",
      "Finish: 2016-06-23/2016-06-23:01:34:01\n",
      "2016-06-23/2016-06-23:04:34:01\n",
      "Finish: 2016-06-23/2016-06-23:04:34:01\n",
      "2016-06-23/2016-06-23:18:34:01\n",
      "Finish: 2016-06-23/2016-06-23:18:34:01\n",
      "2016-06-23/2016-06-23:09:34:01\n",
      "Finish: 2016-06-23/2016-06-23:09:34:01\n",
      "2016-06-23/2016-06-23:22:34:01\n",
      "Finish: 2016-06-23/2016-06-23:22:34:01\n",
      "2016-06-23/2016-06-23:15:34:01\n",
      "Finish: 2016-06-23/2016-06-23:15:34:01\n",
      "2016-06-23/2016-06-23:12:34:01\n",
      "Finish: 2016-06-23/2016-06-23:12:34:01\n",
      "2016-06-23/2016-06-23:00:34:02\n",
      "Finish: 2016-06-23/2016-06-23:00:34:02\n",
      "2016-06-23/2016-06-23:03:34:01\n",
      "Finish: 2016-06-23/2016-06-23:03:34:01\n",
      "2016-06-23/2016-06-23_23_34_01\n",
      "Finish: 2016-06-23/2016-06-23_23_34_01\n",
      "2016-06-23/2016-06-23:06:34:01\n",
      "Finish: 2016-06-23/2016-06-23:06:34:01\n",
      "2016-06-23/2016-06-23:17:34:01\n",
      "Finish: 2016-06-23/2016-06-23:17:34:01\n",
      "2016-06-23/2016-06-23:20:34:01\n",
      "Finish: 2016-06-23/2016-06-23:20:34:01\n",
      "2016-06-23/2016-06-23:05:34:01\n",
      "Finish: 2016-06-23/2016-06-23:05:34:01\n",
      "2016-06-23/2016-06-23:19:34:01\n",
      "Finish: 2016-06-23/2016-06-23:19:34:01\n",
      "2016-06-23/2016-06-23:08:34:01\n",
      "Finish: 2016-06-23/2016-06-23:08:34:01\n",
      "2016-06-23/2016-06-23:14:34:01\n",
      "Finish: 2016-06-23/2016-06-23:14:34:01\n",
      "2016-06-23/2016-06-23:11:34:01\n",
      "Finish: 2016-06-23/2016-06-23:11:34:01\n",
      "2016-06-23/2016-06-23:07:34:01\n",
      "Finish: 2016-06-23/2016-06-23:07:34:01\n",
      "2016-06-23/2016-06-23:16:34:01\n",
      "Finish: 2016-06-23/2016-06-23:16:34:01\n",
      "2016-06-23/2016-06-23:21:34:01\n",
      "Finish: 2016-06-23/2016-06-23:21:34:01\n",
      "2016-06-23/2016-06-23:13:34:01\n",
      "Finish: 2016-06-23/2016-06-23:13:34:01\n",
      "2016-06-23/2016-06-23:02:34:01\n",
      "Finish: 2016-06-23/2016-06-23:02:34:01\n",
      "2017-09-22/2017-09-22:12:05:01\n",
      "Finish: 2017-09-22/2017-09-22:12:05:01\n",
      "2017-09-22/2017-09-22:03:05:01\n",
      "Finish: 2017-09-22/2017-09-22:03:05:01\n",
      "2017-09-22/2017-09-22:06:05:01\n",
      "Finish: 2017-09-22/2017-09-22:06:05:01\n",
      "2017-09-22/2017-09-22:20:05:01\n",
      "Finish: 2017-09-22/2017-09-22:20:05:01\n",
      "2017-09-22/2017-09-22:17:05:01\n",
      "Finish: 2017-09-22/2017-09-22:17:05:01\n",
      "2017-09-22/2017-09-22:05:05:02\n",
      "Finish: 2017-09-22/2017-09-22:05:05:02\n",
      "2017-09-22/2017-09-22:01:05:01\n",
      "Finish: 2017-09-22/2017-09-22:01:05:01\n",
      "2017-09-22/2017-09-22:18:05:01\n",
      "Finish: 2017-09-22/2017-09-22:18:05:01\n",
      "2017-09-22/2017-09-22:04:05:01\n",
      "Finish: 2017-09-22/2017-09-22:04:05:01\n",
      "2017-09-22/2017-09-22:15:05:01\n",
      "Finish: 2017-09-22/2017-09-22:15:05:01\n",
      "2017-09-22/2017-09-22:22:05:01\n",
      "Finish: 2017-09-22/2017-09-22:22:05:01\n",
      "2017-09-22/2017-09-22:09:05:01\n",
      "Finish: 2017-09-22/2017-09-22:09:05:01\n",
      "2017-09-22/2017-09-22:07:05:01\n",
      "Finish: 2017-09-22/2017-09-22:07:05:01\n",
      "2017-09-22/2017-09-22:21:05:01\n",
      "Finish: 2017-09-22/2017-09-22:21:05:01\n",
      "2017-09-22/2017-09-22:16:05:01\n",
      "Finish: 2017-09-22/2017-09-22:16:05:01\n",
      "2017-09-22/2017-09-22:13:05:01\n",
      "Finish: 2017-09-22/2017-09-22:13:05:01\n",
      "2017-09-22/2017-09-22:10:05:02\n",
      "Finish: 2017-09-22/2017-09-22:10:05:02\n",
      "2017-09-22/2017-09-22:02:05:01\n",
      "Finish: 2017-09-22/2017-09-22:02:05:01\n",
      "2017-09-22/2017-09-22:19:05:01\n",
      "Finish: 2017-09-22/2017-09-22:19:05:01\n",
      "2017-09-22/2017-09-22:14:05:01\n",
      "Finish: 2017-09-22/2017-09-22:14:05:01\n",
      "2017-09-22/2017-09-22:23:05:01\n",
      "Finish: 2017-09-22/2017-09-22:23:05:01\n",
      "2017-09-22/2017-09-22:08:05:01\n",
      "Finish: 2017-09-22/2017-09-22:08:05:01\n",
      "2017-09-22/2017-09-22:11:05:01\n",
      "Finish: 2017-09-22/2017-09-22:11:05:01\n",
      "2017-09-22/2017-09-22:00:05:01\n",
      "Finish: 2017-09-22/2017-09-22:00:05:01\n",
      "2017-09-23/2017-09-23:11:05:01\n",
      "Finish: 2017-09-23/2017-09-23:11:05:01\n",
      "2017-09-23/2017-09-23:00:05:01\n",
      "Finish: 2017-09-23/2017-09-23:00:05:01\n",
      "2017-09-23/2017-09-23:19:05:01\n",
      "Finish: 2017-09-23/2017-09-23:19:05:01\n",
      "2017-09-23/2017-09-23:08:05:01\n",
      "Finish: 2017-09-23/2017-09-23:08:05:01\n",
      "2017-09-23/2017-09-23:23:05:01\n",
      "Finish: 2017-09-23/2017-09-23:23:05:01\n",
      "2017-09-23/2017-09-23:14:05:01\n",
      "Finish: 2017-09-23/2017-09-23:14:05:01\n",
      "2017-09-23/2017-09-23:02:05:01\n",
      "Finish: 2017-09-23/2017-09-23:02:05:01\n",
      "2017-09-23/2017-09-23:07:05:01\n",
      "Finish: 2017-09-23/2017-09-23:07:05:01\n",
      "2017-09-23/2017-09-23:16:05:01\n",
      "Finish: 2017-09-23/2017-09-23:16:05:01\n",
      "2017-09-23/2017-09-23:21:05:01\n",
      "Finish: 2017-09-23/2017-09-23:21:05:01\n",
      "2017-09-23/2017-09-23:04:05:01\n",
      "Finish: 2017-09-23/2017-09-23:04:05:01\n",
      "2017-09-23/2017-09-23:18:05:01\n",
      "Finish: 2017-09-23/2017-09-23:18:05:01\n",
      "2017-09-23/2017-09-23:09:05:01\n",
      "Finish: 2017-09-23/2017-09-23:09:05:01\n",
      "2017-09-23/2017-09-23:22:05:01\n",
      "Finish: 2017-09-23/2017-09-23:22:05:01\n",
      "2017-09-23/2017-09-23:15:05:01\n",
      "Finish: 2017-09-23/2017-09-23:15:05:01\n",
      "2017-09-23/2017-09-23:10:05:01\n",
      "Finish: 2017-09-23/2017-09-23:10:05:01\n",
      "2017-09-23/2017-09-23:01:05:01\n",
      "Finish: 2017-09-23/2017-09-23:01:05:01\n",
      "2017-09-23/2017-09-23:13:05:02\n",
      "Finish: 2017-09-23/2017-09-23:13:05:02\n",
      "2017-09-23/2017-09-23:06:05:01\n",
      "Finish: 2017-09-23/2017-09-23:06:05:01\n",
      "2017-09-23/2017-09-23:05:05:02\n",
      "Finish: 2017-09-23/2017-09-23:05:05:02\n",
      "2017-09-23/2017-09-23:17:05:01\n",
      "Finish: 2017-09-23/2017-09-23:17:05:01\n",
      "2017-09-23/2017-09-23:20:05:01\n",
      "Finish: 2017-09-23/2017-09-23:20:05:01\n",
      "2017-09-23/2017-09-23:12:05:01\n",
      "Finish: 2017-09-23/2017-09-23:12:05:01\n",
      "2017-09-23/2017-09-23:03:05:01\n",
      "Finish: 2017-09-23/2017-09-23:03:05:01\n",
      "Finish ALL\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def get_paths(directory_path):\n",
    "    '''\n",
    "        input: directory that stores twitter data\n",
    "        output: paths of all twitter json. e.g., 2016-06-21/2016-06-21:18:34:02\n",
    "    '''\n",
    "    paths = []\n",
    "    for file_date in listdir(directory_path):\n",
    "        if file_date[0]!='.':\n",
    "            if file_date[0:1] =='2':\n",
    "                for file_hour in listdir(join(directory_path,file_date)):\n",
    "                    if file_hour[0]!='.':\n",
    "                        file_hour = file_hour.split('.json')[0]  \n",
    "                        paths.append(file_date+'/'+file_hour)\n",
    "                    else:pass\n",
    "            else:pass \n",
    "        else:pass \n",
    "    return paths\n",
    "    \n",
    "    \n",
    "def parse_attr(datajson, attr):\n",
    "    '''\n",
    "        input: level0-level1-level2, e.g., 'id', 'retweeted_status-created_at','retweeted_status-user-statuses_count'\n",
    "        output: attr's list\n",
    "        attention: attr-'entities-hashtags' has special structure\n",
    "    '''\n",
    "    attr_split = attr.split('-')\n",
    "    \n",
    "    if len(attr_split)==1:\n",
    "        return datajson[attr_split[0]]\n",
    "    elif len(attr_split)==2:   \n",
    "        # deal with some special cases, e.g., entities-hashtags\n",
    "        values = datajson[attr_split[0]][attr_split[1]]\n",
    "        if(attr=='entities-hashtags'):\n",
    "            hashtags = [hashtag['text'] for hashtag in values]\n",
    "            if(len(hashtags)!=0):\n",
    "                return reduce((lambda x,y: x+', '+y),hashtags )\n",
    "            else:\n",
    "                return ''\n",
    "        else:    \n",
    "            return values   \n",
    "    elif len(attr_split)==3:\n",
    "        return datajson[attr_split[0]][attr_split[1]][attr_split[2]]\n",
    "    \n",
    "    else:pass\n",
    "    \n",
    "    \n",
    "paths = get_paths('../twitter')\n",
    "for path in paths:\n",
    "    with open('../twitter/'+path+'.json', 'r+') as r:\n",
    "        data = []\n",
    "        columns = ['id','geo','text',\n",
    "                   'entities-hashtags',\n",
    "                   'created_at',\n",
    "                   'retweeted_status-created_at',\n",
    "                   'retweeted_status-retweet_count',\n",
    "                   'retweeted_status-favorite_count',\n",
    "                   'retweeted_status-user-friends_count',\n",
    "                   'retweeted_status-user-favourites_count',\n",
    "                   'retweeted_status-user-statuses_count',\n",
    "                   'retweeted_status-user-followers_count',\n",
    "                   'user-friends_count','user-favourites_count','user-statuses_count',\n",
    "                   'user-geo_enabled','user-followers_count','user-location']\n",
    "        \n",
    "        for index, line in enumerate(r.readlines()):\n",
    "            data.append([]) #i.e., create data[index] = []\n",
    "            d = json.loads(line) \n",
    "            for attr in columns:\n",
    "                try:\n",
    "                    attr_value = parse_attr(d,attr)\n",
    "                except KeyError:\n",
    "                    attr_value = 'NULL'\n",
    "                data[index].append(attr_value)\n",
    "                \n",
    "                \n",
    "        # build json that is used for pd.read_json function\n",
    "        json_for_pd = dict()\n",
    "        json_for_pd['columns'] = columns\n",
    "        json_for_pd['index'] = [i for i in range(len(data))]\n",
    "        json_for_pd['data'] = data\n",
    "        # write json\n",
    "        with open('../cleaned_twitter/'+path[:11]+'c-'+path[11:].replace('/','-').replace(':','-')+'.json','w+') as w:\n",
    "            w.write(json.dumps(json_for_pd))\n",
    "        print('Finish: {}'.format(path))\n",
    "        \n",
    "print('Finish ALL')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id   geo  \\\n",
      "0  745308821631995904  None   \n",
      "1  745308821690683393  None   \n",
      "2  745308824224030720  None   \n",
      "3  745308824408563713  None   \n",
      "4  745308824731529216  None   \n",
      "\n",
      "                                                text  \\\n",
      "0  RT @gupta_james: Lily Allen called me and 30 m...   \n",
      "1  RT @Fight4UK: #Brexit ~ #GO ~ #LeaveEU ~ #Vote...   \n",
      "2  RT @me4ukip: Another reason to #VoteLeave http...   \n",
      "3  RT @ohchrisburton: Butterflies only live for t...   \n",
      "4  RT @Jade_S97: Is there an award for the most v...   \n",
      "\n",
      "                              entities-hashtags          created_at  \\\n",
      "0                                     VoteLeave 2016-06-21 17:34:04   \n",
      "1  Brexit, GO, LeaveEU, VoteLeave, BetterOffOut 2016-06-21 17:34:04   \n",
      "2                                     VoteLeave 2016-06-21 17:34:05   \n",
      "3                                               2016-06-21 17:34:05   \n",
      "4                        VoteLeave, TakeControl 2016-06-21 17:34:05   \n",
      "\n",
      "      retweeted_status-created_at retweeted_status-retweet_count  \\\n",
      "0  Tue Jun 21 12:04:23 +0000 2016                            454   \n",
      "1  Tue Jun 21 17:26:44 +0000 2016                              3   \n",
      "2  Tue Jun 21 17:32:18 +0000 2016                              1   \n",
      "3  Sun Jun 19 19:07:07 +0000 2016                           1728   \n",
      "4  Mon Jun 20 14:18:01 +0000 2016                            291   \n",
      "\n",
      "  retweeted_status-favorite_count retweeted_status-user-friends_count  \\\n",
      "0                             428                                2564   \n",
      "1                               2                               12756   \n",
      "2                               0                                1085   \n",
      "3                            2133                                2119   \n",
      "4                             388                                 230   \n",
      "\n",
      "  retweeted_status-user-favourites_count retweeted_status-user-statuses_count  \\\n",
      "0                                   2006                                 4253   \n",
      "1                                  39879                               178322   \n",
      "2                                   5485                                 6156   \n",
      "3                                  79137                               106322   \n",
      "4                                   4126                                 7275   \n",
      "\n",
      "  retweeted_status-user-followers_count  user-friends_count  \\\n",
      "0                                  1800                4137   \n",
      "1                                 13926                1421   \n",
      "2                                  1102                1285   \n",
      "3                                  6119                 263   \n",
      "4                                   496                 278   \n",
      "\n",
      "   user-favourites_count  user-statuses_count  user-geo_enabled  \\\n",
      "0                   5227                36401             False   \n",
      "1                   4167                 7532              True   \n",
      "2                   1542                11247             False   \n",
      "3                  18019               112102              True   \n",
      "4                   1554                 4868             False   \n",
      "\n",
      "   user-followers_count              user-location  \n",
      "0                  5039  Warwickshire, England, UK  \n",
      "1                   869        Mattishall, Norfolk  \n",
      "2                  1016                       None  \n",
      "3                 17253    intersectional feminist  \n",
      "4                   264           Southern England  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_json('../cleaned_twitter/2016-06-21/c-2016-06-21-18-34-02.json', orient='split')\n",
    "\n",
    "print(df.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
